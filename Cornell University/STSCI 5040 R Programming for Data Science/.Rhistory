}
View(df)
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- select(df, 1:3)
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
df <- select(df, 1:3)
df
df <- select(df, 1:2)
df
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
df <- select(df, 1:3)
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
df
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
df
url
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
# Complete the line below by filling it out with your CANDIDATE NUMBER
candidate_number <- 00000
options(warn = -1)
library(tidyverse)
library(tidytext)
library(quanteda)
library(topicmodels)
library(Matrix)
library(reshape)
library(quanteda.textplots)
library(quanteda.textstats)
library(wordcloud)
df <-
read_csv("./zeroshot-twitter-financial-news-topic-recoded.csv")
# Filters the data frame to include only rows with the specified labels in the manually_annotated_label column
new_df <- df %>%
filter(
manually_annotated_label %in% c(
"Dividend",
"Earnings",
"Energy | Oil",
"Legal | Regulation",
"Markets"
)
)
# The text of these rows is extracted and cleaned by removing URLs, excess whitespace, digits, and punctuation.
text <- new_df$text
text <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", text)
text <- gsub("\\s+", " ", text)
text <- gsub('[[:digit:]]+', '', text)
text <- gsub("[[:punct:]]", "", text)
text_df <- tibble(line = 1:length(text), text = text)
# The cleaned text is then tokenized into individual words and joined with the stop_words dataset to remove common English stop words.
tidy_text_df <- text_df %>%
unnest_tokens(word, text)
data("stop_words")
tidy_text_df <- tidy_text_df %>%
anti_join(stop_words)
# A document-term matrix is created from the remaining words.
doc_freq_matrix<- tidy_text_df$word %>%
dfm()
#create 5 topics
# create a seed to make the model's output predictable.
doc_freq_matrix_lda <-
LDA(doc_freq_matrix, k = 5, control = list(seed = 1))
# word_topic probabilities
# use "beta" to extract the per-topic-per-word-probabilities
doc_freq_matrix_topics <- tidy(doc_freq_matrix_lda, matrix = "beta")
# 10 most common term in each topic
doc_freq_matrix_terms <- doc_freq_matrix_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic,-beta)
doc_freq_matrix_terms
doc_freq_matrix_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap( ~ topic, scales = "free") +
scale_y_reordered()
# document topic probabilities
doc_freq_matrix_documents <-
tidy(doc_freq_matrix_lda, matrix = "gamma")
doc_freq_matrix_documents
# selecting one of the topics
# selecting topic 1
doc_freq_matrix_topic_1 <- doc_freq_matrix_topics %>%
filter(topic == 1)
# wordcloud
doc_freq_matrix_topic_1 %>% count(term, sort = T) %>% with(
wordcloud(
term,
n,
max.words = 100,
scale = c(2, 0.25),
min.freq = 10,
random.order = F,
random.color = T,
colors = brewer.pal(9, "Purples")[5:9]
)
)
# keyness for selected topics
tstat_topic1 <- textstat_keyness(doc_freq_matrix, target = "text1")
textplot_keyness(tstat_topic1, margin = 0.2, n = 30)
# scatterplot of dataset colored words in the topic
doc_freq_matrix_topics %>%
filter(beta >= median(beta)) %>%
mutate(
topic = factor(topic),
topic = fct_recode(
topic,
"Dividend" = "1",
"Earnings" = "2",
"Energy | Oil" = "3",
"Legal | Regulation" = "4",
"Markets" = "5"
)
) %>%
ggplot(aes(x = term, y = beta, color = topic)) +
geom_point()+
theme(axis.text.x = element_text(
angle = 90, vjust = 0.5, hjust = 1
)) +
scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
View(doc_freq_matrix_topics)
View(doc_freq_matrix_topics)
View(tstat_topic1)
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
# combine data frames into a single data frame
combined_df <- bind_rows(data_frames)
df
View(data_frames)
setwd("~/Documents/Projects/LEAVES")
View(data_frames)
names(df)[0]
names(df)
names(df)[[0]]
names(df)[0]
"" %in% names(df)
"LENGTH" %in% names(df)
library(rvest)
quadHandle <- function(x){
if_else(grepl("AG", x, fixed=TRUE), 'AG', "ART")
}
base_URL = "http://jeremyentner.com/3040_Final_Data/"
# create empty list to store data frames
data_frames <- list()
hrefs <- scrape_hrefs("http://jeremyentner.com/3040_Final_Data/HtmlExampleScraping2.html")
# loop through URLs and read in each file as a data frame
for (url in hrefs) {
first_year = substring(url, 0, 4)
last_year = sub(".csv", "", substrRight(url, 8))
if(startsWith(url, '2021') || startsWith(url, '2022')){
df <- read_csv(paste(c(base_URL, url), collapse=''))
if(nrow(df) < 30){
df <- read_csv(paste(c(base_URL, url), collapse=''), col_names = c('length','shape','date'))
}
if(ncol(df) > 2) {
if ("" %in% names(df)){
df <- select(df, 2:4)
}
df <- select(df, 1:3)
}
names(df) <- c('length','shape','date')
quad <- sub(".CSV", "", strsplit(url, "/")[[1]][2])
section <- sub(".CSV", "", strsplit(strsplit(url, "/")[[1]][2], "_")[[1]][2])
df <- mutate(df, year = first_year, quad = quad, section = section, length = as.numeric(gsub("[^0-9.-]", "", length)))
} else {
df <- read_csv(paste(c(base_URL, url), collapse=''))
df <- mutate(df, year = last_year)
}
names(df) <- gsub(" ", "_", toupper(names(df)))
if("LAB_SECTION" %in% names(df)){
df <- mutate(df, SECTION = LAB_SECTION)
}
if (last_year == '2019') {
df <- mutate(df, SHAPE = as.character(T_NT), LENGTH = LENGTHS, QUAD = AG_ART)
}
df <- mutate(df, QUAD = quadHandle(QUAD), SECTION = as.factor(SECTION))
data_frames[[url]] <- df
}
# combine data frames into a single data frame
combined_df <- bind_rows(data_frames)
View(data_frames)
hrefs
